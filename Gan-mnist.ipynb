{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train (60000, 28, 28) (60000,)\n",
      "Test (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "# example of loading the mnist dataset\n",
    "from keras.datasets.mnist import load_data\n",
    "# load the 28x28 images into memory\n",
    "(trainX, trainy), (testX, testy) = load_data()\n",
    "# summarize the shape of the dataset\n",
    "print('Train', trainX.shape, trainy.shape)\n",
    "print('Test', testX.shape, testy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/keras/engine/training.py:973: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">1, 1/468, d=0.673, g=0.754\n",
      ">1, 2/468, d=0.665, g=0.759\n",
      ">1, 3/468, d=0.657, g=0.771\n",
      ">1, 4/468, d=0.650, g=0.781\n",
      ">1, 5/468, d=0.646, g=0.781\n",
      ">1, 6/468, d=0.645, g=0.774\n",
      ">1, 7/468, d=0.648, g=0.761\n",
      ">1, 8/468, d=0.655, g=0.743\n",
      ">1, 9/468, d=0.651, g=0.728\n",
      ">1, 10/468, d=0.654, g=0.714\n",
      ">1, 11/468, d=0.660, g=0.710\n",
      ">1, 12/468, d=0.654, g=0.708\n",
      ">1, 13/468, d=0.651, g=0.708\n",
      ">1, 14/468, d=0.654, g=0.709\n",
      ">1, 15/468, d=0.646, g=0.710\n",
      ">1, 16/468, d=0.639, g=0.711\n",
      ">1, 17/468, d=0.634, g=0.712\n",
      ">1, 18/468, d=0.631, g=0.713\n",
      ">1, 19/468, d=0.638, g=0.715\n",
      ">1, 20/468, d=0.620, g=0.717\n",
      ">1, 21/468, d=0.617, g=0.718\n",
      ">1, 22/468, d=0.601, g=0.720\n",
      ">1, 23/468, d=0.600, g=0.723\n",
      ">1, 24/468, d=0.604, g=0.724\n",
      ">1, 25/468, d=0.598, g=0.727\n",
      ">1, 26/468, d=0.588, g=0.730\n",
      ">1, 27/468, d=0.580, g=0.735\n",
      ">1, 28/468, d=0.572, g=0.740\n",
      ">1, 29/468, d=0.575, g=0.744\n",
      ">1, 30/468, d=0.559, g=0.750\n",
      ">1, 31/468, d=0.552, g=0.756\n",
      ">1, 32/468, d=0.543, g=0.762\n",
      ">1, 33/468, d=0.534, g=0.771\n",
      ">1, 34/468, d=0.521, g=0.775\n",
      ">1, 35/468, d=0.512, g=0.783\n",
      ">1, 36/468, d=0.511, g=0.793\n",
      ">1, 37/468, d=0.493, g=0.801\n",
      ">1, 38/468, d=0.472, g=0.811\n",
      ">1, 39/468, d=0.467, g=0.819\n",
      ">1, 40/468, d=0.450, g=0.829\n",
      ">1, 41/468, d=0.455, g=0.842\n",
      ">1, 42/468, d=0.436, g=0.853\n",
      ">1, 43/468, d=0.426, g=0.867\n",
      ">1, 44/468, d=0.416, g=0.880\n",
      ">1, 45/468, d=0.405, g=0.893\n",
      ">1, 46/468, d=0.385, g=0.908\n",
      ">1, 47/468, d=0.378, g=0.923\n",
      ">1, 48/468, d=0.359, g=0.941\n",
      ">1, 49/468, d=0.347, g=0.958\n",
      ">1, 50/468, d=0.332, g=0.977\n",
      ">1, 51/468, d=0.324, g=0.998\n",
      ">1, 52/468, d=0.320, g=1.018\n",
      ">1, 53/468, d=0.301, g=1.042\n",
      ">1, 54/468, d=0.290, g=1.066\n",
      ">1, 55/468, d=0.283, g=1.089\n",
      ">1, 56/468, d=0.264, g=1.121\n",
      ">1, 57/468, d=0.252, g=1.145\n",
      ">1, 58/468, d=0.240, g=1.175\n",
      ">1, 59/468, d=0.229, g=1.203\n",
      ">1, 60/468, d=0.221, g=1.238\n",
      ">1, 61/468, d=0.207, g=1.274\n",
      ">1, 62/468, d=0.200, g=1.305\n",
      ">1, 63/468, d=0.191, g=1.354\n",
      ">1, 64/468, d=0.178, g=1.387\n",
      ">1, 65/468, d=0.171, g=1.435\n",
      ">1, 66/468, d=0.161, g=1.470\n",
      ">1, 67/468, d=0.155, g=1.518\n",
      ">1, 68/468, d=0.146, g=1.557\n",
      ">1, 69/468, d=0.134, g=1.598\n",
      ">1, 70/468, d=0.129, g=1.655\n",
      ">1, 71/468, d=0.118, g=1.705\n",
      ">1, 72/468, d=0.111, g=1.748\n",
      ">1, 73/468, d=0.104, g=1.802\n",
      ">1, 74/468, d=0.101, g=1.857\n",
      ">1, 75/468, d=0.091, g=1.910\n",
      ">1, 76/468, d=0.086, g=1.960\n",
      ">1, 77/468, d=0.085, g=2.015\n",
      ">1, 78/468, d=0.075, g=2.074\n",
      ">1, 79/468, d=0.074, g=2.127\n",
      ">1, 80/468, d=0.069, g=2.184\n",
      ">1, 81/468, d=0.064, g=2.239\n",
      ">1, 82/468, d=0.060, g=2.301\n",
      ">1, 83/468, d=0.057, g=2.373\n",
      ">1, 84/468, d=0.054, g=2.411\n",
      ">1, 85/468, d=0.049, g=2.475\n",
      ">1, 86/468, d=0.047, g=2.531\n",
      ">1, 87/468, d=0.043, g=2.589\n",
      ">1, 88/468, d=0.042, g=2.648\n",
      ">1, 89/468, d=0.039, g=2.692\n",
      ">1, 90/468, d=0.036, g=2.757\n",
      ">1, 91/468, d=0.035, g=2.834\n",
      ">1, 92/468, d=0.033, g=2.869\n",
      ">1, 93/468, d=0.030, g=2.926\n",
      ">1, 94/468, d=0.029, g=2.971\n",
      ">1, 95/468, d=0.027, g=3.028\n",
      ">1, 96/468, d=0.026, g=3.086\n",
      ">1, 97/468, d=0.024, g=3.128\n",
      ">1, 98/468, d=0.023, g=3.193\n",
      ">1, 99/468, d=0.022, g=3.250\n",
      ">1, 100/468, d=0.021, g=3.287\n",
      ">1, 101/468, d=0.019, g=3.331\n",
      ">1, 102/468, d=0.019, g=3.382\n",
      ">1, 103/468, d=0.018, g=3.426\n",
      ">1, 104/468, d=0.017, g=3.491\n",
      ">1, 105/468, d=0.016, g=3.534\n",
      ">1, 106/468, d=0.015, g=3.548\n",
      ">1, 107/468, d=0.015, g=3.600\n",
      ">1, 108/468, d=0.014, g=3.655\n",
      ">1, 109/468, d=0.014, g=3.687\n",
      ">1, 110/468, d=0.013, g=3.738\n",
      ">1, 111/468, d=0.012, g=3.777\n",
      ">1, 112/468, d=0.012, g=3.817\n",
      ">1, 113/468, d=0.011, g=3.859\n",
      ">1, 114/468, d=0.011, g=3.893\n",
      ">1, 115/468, d=0.011, g=3.918\n",
      ">1, 116/468, d=0.010, g=3.972\n",
      ">1, 117/468, d=0.010, g=4.002\n",
      ">1, 118/468, d=0.010, g=4.045\n",
      ">1, 119/468, d=0.009, g=4.060\n",
      ">1, 120/468, d=0.009, g=4.098\n",
      ">1, 121/468, d=0.009, g=4.149\n",
      ">1, 122/468, d=0.008, g=4.167\n",
      ">1, 123/468, d=0.008, g=4.179\n",
      ">1, 124/468, d=0.008, g=4.223\n",
      ">1, 125/468, d=0.008, g=4.250\n",
      ">1, 126/468, d=0.007, g=4.284\n",
      ">1, 127/468, d=0.007, g=4.307\n",
      ">1, 128/468, d=0.007, g=4.342\n",
      ">1, 129/468, d=0.007, g=4.362\n",
      ">1, 130/468, d=0.007, g=4.398\n",
      ">1, 131/468, d=0.006, g=4.418\n",
      ">1, 132/468, d=0.006, g=4.423\n",
      ">1, 133/468, d=0.006, g=4.472\n",
      ">1, 134/468, d=0.006, g=4.488\n",
      ">1, 135/468, d=0.006, g=4.540\n",
      ">1, 136/468, d=0.006, g=4.539\n",
      ">1, 137/468, d=0.006, g=4.573\n",
      ">1, 138/468, d=0.005, g=4.585\n",
      ">1, 139/468, d=0.005, g=4.609\n",
      ">1, 140/468, d=0.005, g=4.657\n",
      ">1, 141/468, d=0.005, g=4.641\n",
      ">1, 142/468, d=0.005, g=4.683\n",
      ">1, 143/468, d=0.005, g=4.692\n",
      ">1, 144/468, d=0.005, g=4.727\n",
      ">1, 145/468, d=0.005, g=4.741\n",
      ">1, 146/468, d=0.005, g=4.740\n",
      ">1, 147/468, d=0.004, g=4.815\n",
      ">1, 148/468, d=0.004, g=4.813\n",
      ">1, 149/468, d=0.004, g=4.804\n",
      ">1, 150/468, d=0.004, g=4.832\n",
      ">1, 151/468, d=0.004, g=4.868\n",
      ">1, 152/468, d=0.004, g=4.894\n",
      ">1, 153/468, d=0.004, g=4.917\n",
      ">1, 154/468, d=0.004, g=4.911\n",
      ">1, 155/468, d=0.004, g=4.949\n",
      ">1, 156/468, d=0.004, g=4.956\n",
      ">1, 157/468, d=0.004, g=4.991\n",
      ">1, 158/468, d=0.004, g=5.010\n",
      ">1, 159/468, d=0.003, g=5.014\n",
      ">1, 160/468, d=0.003, g=5.038\n",
      ">1, 161/468, d=0.003, g=5.038\n",
      ">1, 162/468, d=0.003, g=5.090\n",
      ">1, 163/468, d=0.003, g=5.092\n",
      ">1, 164/468, d=0.003, g=5.085\n",
      ">1, 165/468, d=0.003, g=5.121\n",
      ">1, 166/468, d=0.003, g=5.162\n",
      ">1, 167/468, d=0.003, g=5.134\n",
      ">1, 168/468, d=0.003, g=5.178\n",
      ">1, 169/468, d=0.003, g=5.206\n",
      ">1, 170/468, d=0.003, g=5.189\n",
      ">1, 171/468, d=0.003, g=5.211\n",
      ">1, 172/468, d=0.003, g=5.222\n",
      ">1, 173/468, d=0.003, g=5.243\n",
      ">1, 174/468, d=0.003, g=5.267\n",
      ">1, 175/468, d=0.003, g=5.293\n",
      ">1, 176/468, d=0.003, g=5.295\n",
      ">1, 177/468, d=0.003, g=5.326\n",
      ">1, 178/468, d=0.003, g=5.334\n",
      ">1, 179/468, d=0.002, g=5.321\n",
      ">1, 180/468, d=0.002, g=5.360\n",
      ">1, 181/468, d=0.002, g=5.367\n",
      ">1, 182/468, d=0.002, g=5.402\n",
      ">1, 183/468, d=0.002, g=5.415\n",
      ">1, 184/468, d=0.002, g=5.412\n",
      ">1, 185/468, d=0.002, g=5.437\n",
      ">1, 186/468, d=0.002, g=5.427\n",
      ">1, 187/468, d=0.002, g=5.461\n",
      ">1, 188/468, d=0.002, g=5.470\n",
      ">1, 189/468, d=0.002, g=5.464\n",
      ">1, 190/468, d=0.002, g=5.482\n",
      ">1, 191/468, d=0.002, g=5.509\n",
      ">1, 192/468, d=0.002, g=5.532\n",
      ">1, 193/468, d=0.002, g=5.546\n",
      ">1, 194/468, d=0.002, g=5.553\n",
      ">1, 195/468, d=0.002, g=5.549\n",
      ">1, 196/468, d=0.002, g=5.591\n",
      ">1, 197/468, d=0.002, g=5.580\n",
      ">1, 198/468, d=0.002, g=5.615\n",
      ">1, 199/468, d=0.002, g=5.608\n",
      ">1, 200/468, d=0.002, g=5.620\n",
      ">1, 201/468, d=0.002, g=5.650\n",
      ">1, 202/468, d=0.002, g=5.668\n",
      ">1, 203/468, d=0.002, g=5.653\n",
      ">1, 204/468, d=0.002, g=5.675\n",
      ">1, 205/468, d=0.002, g=5.685\n",
      ">1, 206/468, d=0.002, g=5.706\n",
      ">1, 207/468, d=0.002, g=5.719\n",
      ">1, 208/468, d=0.002, g=5.740\n",
      ">1, 209/468, d=0.002, g=5.724\n",
      ">1, 210/468, d=0.002, g=5.755\n",
      ">1, 211/468, d=0.002, g=5.739\n",
      ">1, 212/468, d=0.002, g=5.751\n",
      ">1, 213/468, d=0.002, g=5.779\n",
      ">1, 214/468, d=0.002, g=5.777\n",
      ">1, 215/468, d=0.002, g=5.836\n",
      ">1, 216/468, d=0.002, g=5.814\n",
      ">1, 217/468, d=0.002, g=5.801\n",
      ">1, 218/468, d=0.002, g=5.851\n",
      ">1, 219/468, d=0.001, g=5.833\n",
      ">1, 220/468, d=0.002, g=5.839\n",
      ">1, 221/468, d=0.001, g=5.881\n",
      ">1, 222/468, d=0.001, g=5.874\n",
      ">1, 223/468, d=0.001, g=5.900\n",
      ">1, 224/468, d=0.001, g=5.922\n",
      ">1, 225/468, d=0.001, g=5.902\n",
      ">1, 226/468, d=0.001, g=5.917\n",
      ">1, 227/468, d=0.001, g=5.947\n",
      ">1, 228/468, d=0.001, g=5.951\n",
      ">1, 229/468, d=0.001, g=5.972\n",
      ">1, 230/468, d=0.001, g=5.967\n",
      ">1, 231/468, d=0.001, g=5.961\n",
      ">1, 232/468, d=0.001, g=5.975\n",
      ">1, 233/468, d=0.001, g=5.993\n",
      ">1, 234/468, d=0.001, g=6.012\n",
      ">1, 235/468, d=0.001, g=6.012\n",
      ">1, 236/468, d=0.001, g=6.038\n",
      ">1, 237/468, d=0.001, g=6.052\n",
      ">1, 238/468, d=0.001, g=6.036\n",
      ">1, 239/468, d=0.001, g=6.076\n",
      ">1, 240/468, d=0.001, g=6.103\n",
      ">1, 241/468, d=0.001, g=6.083\n",
      ">1, 242/468, d=0.001, g=6.088\n",
      ">1, 243/468, d=0.001, g=6.121\n",
      ">1, 244/468, d=0.001, g=6.112\n",
      ">1, 245/468, d=0.001, g=6.076\n",
      ">1, 246/468, d=0.001, g=6.123\n",
      ">1, 247/468, d=0.001, g=6.129\n",
      ">1, 248/468, d=0.001, g=6.154\n",
      ">1, 249/468, d=0.001, g=6.169\n",
      ">1, 250/468, d=0.001, g=6.177\n",
      ">1, 251/468, d=0.001, g=6.158\n",
      ">1, 252/468, d=0.001, g=6.192\n",
      ">1, 253/468, d=0.001, g=6.197\n",
      ">1, 254/468, d=0.001, g=6.208\n",
      ">1, 255/468, d=0.001, g=6.239\n",
      ">1, 256/468, d=0.001, g=6.222\n",
      ">1, 257/468, d=0.001, g=6.243\n",
      ">1, 258/468, d=0.001, g=6.231\n",
      ">1, 259/468, d=0.001, g=6.261\n",
      ">1, 260/468, d=0.001, g=6.254\n",
      ">1, 261/468, d=0.001, g=6.255\n",
      ">1, 262/468, d=0.001, g=6.266\n",
      ">1, 263/468, d=0.001, g=6.278\n",
      ">1, 264/468, d=0.001, g=6.304\n",
      ">1, 265/468, d=0.001, g=6.304\n",
      ">1, 266/468, d=0.001, g=6.298\n",
      ">1, 267/468, d=0.001, g=6.304\n",
      ">1, 268/468, d=0.001, g=6.335\n",
      ">1, 269/468, d=0.001, g=6.334\n",
      ">1, 270/468, d=0.001, g=6.338\n",
      ">1, 271/468, d=0.001, g=6.361\n",
      ">1, 272/468, d=0.001, g=6.356\n",
      ">1, 273/468, d=0.001, g=6.388\n",
      ">1, 274/468, d=0.001, g=6.373\n",
      ">1, 275/468, d=0.001, g=6.412\n",
      ">1, 276/468, d=0.001, g=6.402\n",
      ">1, 277/468, d=0.001, g=6.398\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">1, 278/468, d=0.001, g=6.420\n",
      ">1, 279/468, d=0.001, g=6.416\n",
      ">1, 280/468, d=0.001, g=6.464\n",
      ">1, 281/468, d=0.001, g=6.444\n",
      ">1, 282/468, d=0.001, g=6.426\n",
      ">1, 283/468, d=0.001, g=6.454\n",
      ">1, 284/468, d=0.001, g=6.474\n",
      ">1, 285/468, d=0.001, g=6.472\n",
      ">1, 286/468, d=0.001, g=6.468\n",
      ">1, 287/468, d=0.001, g=6.510\n",
      ">1, 288/468, d=0.001, g=6.507\n",
      ">1, 289/468, d=0.001, g=6.509\n",
      ">1, 290/468, d=0.001, g=6.520\n",
      ">1, 291/468, d=0.001, g=6.521\n",
      ">1, 292/468, d=0.001, g=6.529\n",
      ">1, 293/468, d=0.001, g=6.541\n",
      ">1, 294/468, d=0.001, g=6.545\n",
      ">1, 295/468, d=0.001, g=6.556\n",
      ">1, 296/468, d=0.001, g=6.557\n",
      ">1, 297/468, d=0.001, g=6.573\n",
      ">1, 298/468, d=0.001, g=6.594\n",
      ">1, 299/468, d=0.001, g=6.578\n",
      ">1, 300/468, d=0.001, g=6.594\n",
      ">1, 301/468, d=0.001, g=6.615\n",
      ">1, 302/468, d=0.001, g=6.605\n",
      ">1, 303/468, d=0.001, g=6.617\n",
      ">1, 304/468, d=0.001, g=6.630\n",
      ">1, 305/468, d=0.001, g=6.677\n",
      ">1, 306/468, d=0.001, g=6.621\n",
      ">1, 307/468, d=0.001, g=6.641\n",
      ">1, 308/468, d=0.001, g=6.683\n",
      ">1, 309/468, d=0.001, g=6.663\n",
      ">1, 310/468, d=0.001, g=6.640\n",
      ">1, 311/468, d=0.001, g=6.659\n",
      ">1, 312/468, d=0.001, g=6.665\n",
      ">1, 313/468, d=0.001, g=6.654\n",
      ">1, 314/468, d=0.001, g=6.712\n",
      ">1, 315/468, d=0.001, g=6.688\n",
      ">1, 316/468, d=0.001, g=6.749\n",
      ">1, 317/468, d=0.001, g=6.709\n",
      ">1, 318/468, d=0.001, g=6.727\n",
      ">1, 319/468, d=0.001, g=6.728\n",
      ">1, 320/468, d=0.001, g=6.733\n",
      ">1, 321/468, d=0.001, g=6.755\n",
      ">1, 322/468, d=0.001, g=6.723\n",
      ">1, 323/468, d=0.001, g=6.738\n",
      ">1, 324/468, d=0.001, g=6.791\n",
      ">1, 325/468, d=0.001, g=6.759\n",
      ">1, 326/468, d=0.001, g=6.768\n",
      ">1, 327/468, d=0.001, g=6.756\n",
      ">1, 328/468, d=0.001, g=6.786\n",
      ">1, 329/468, d=0.001, g=6.812\n",
      ">1, 330/468, d=0.001, g=6.781\n",
      ">1, 331/468, d=0.001, g=6.822\n",
      ">1, 332/468, d=0.001, g=6.845\n",
      ">1, 333/468, d=0.001, g=6.843\n",
      ">1, 334/468, d=0.001, g=6.853\n",
      ">1, 335/468, d=0.001, g=6.847\n",
      ">1, 336/468, d=0.001, g=6.863\n",
      ">1, 337/468, d=0.001, g=6.853\n",
      ">1, 338/468, d=0.001, g=6.867\n",
      ">1, 339/468, d=0.001, g=6.879\n",
      ">1, 340/468, d=0.001, g=6.844\n",
      ">1, 341/468, d=0.001, g=6.876\n",
      ">1, 342/468, d=0.001, g=6.875\n",
      ">1, 343/468, d=0.001, g=6.915\n",
      ">1, 344/468, d=0.000, g=6.901\n",
      ">1, 345/468, d=0.000, g=6.926\n",
      ">1, 346/468, d=0.001, g=6.938\n",
      ">1, 347/468, d=0.001, g=6.889\n",
      ">1, 348/468, d=0.001, g=6.956\n",
      ">1, 349/468, d=0.001, g=6.929\n",
      ">1, 350/468, d=0.000, g=6.949\n",
      ">1, 351/468, d=0.000, g=6.952\n",
      ">1, 352/468, d=0.000, g=6.969\n",
      ">1, 353/468, d=0.000, g=6.969\n",
      ">1, 354/468, d=0.000, g=6.970\n",
      ">1, 355/468, d=0.000, g=6.975\n",
      ">1, 356/468, d=0.000, g=6.957\n",
      ">1, 357/468, d=0.000, g=6.989\n",
      ">1, 358/468, d=0.000, g=6.972\n",
      ">1, 359/468, d=0.000, g=6.975\n",
      ">1, 360/468, d=0.000, g=6.988\n",
      ">1, 361/468, d=0.000, g=7.031\n",
      ">1, 362/468, d=0.000, g=7.004\n",
      ">1, 363/468, d=0.000, g=7.000\n",
      ">1, 364/468, d=0.000, g=7.014\n",
      ">1, 365/468, d=0.000, g=7.018\n",
      ">1, 366/468, d=0.000, g=7.027\n",
      ">1, 367/468, d=0.000, g=7.034\n",
      ">1, 368/468, d=0.000, g=7.044\n",
      ">1, 369/468, d=0.000, g=7.067\n",
      ">1, 370/468, d=0.000, g=7.072\n",
      ">1, 371/468, d=0.000, g=7.065\n",
      ">1, 372/468, d=0.000, g=7.073\n",
      ">1, 373/468, d=0.000, g=7.075\n",
      ">1, 374/468, d=0.000, g=7.071\n",
      ">1, 375/468, d=0.000, g=7.094\n",
      ">1, 376/468, d=0.000, g=7.118\n",
      ">1, 377/468, d=0.000, g=7.124\n",
      ">1, 378/468, d=0.000, g=7.112\n",
      ">1, 379/468, d=0.000, g=7.136\n",
      ">1, 380/468, d=0.000, g=7.096\n",
      ">1, 381/468, d=0.000, g=7.118\n",
      ">1, 382/468, d=0.000, g=7.138\n",
      ">1, 383/468, d=0.000, g=7.128\n",
      ">1, 384/468, d=0.000, g=7.154\n",
      ">1, 385/468, d=0.000, g=7.154\n",
      ">1, 386/468, d=0.000, g=7.149\n",
      ">1, 387/468, d=0.000, g=7.136\n",
      ">1, 388/468, d=0.000, g=7.187\n",
      ">1, 389/468, d=0.000, g=7.167\n",
      ">1, 390/468, d=0.000, g=7.188\n",
      ">1, 391/468, d=0.000, g=7.173\n",
      ">1, 392/468, d=0.000, g=7.194\n",
      ">1, 393/468, d=0.000, g=7.197\n",
      ">1, 394/468, d=0.000, g=7.186\n",
      ">1, 395/468, d=0.000, g=7.204\n",
      ">1, 396/468, d=0.000, g=7.219\n",
      ">1, 397/468, d=0.000, g=7.231\n",
      ">1, 398/468, d=0.000, g=7.270\n",
      ">1, 399/468, d=0.000, g=7.205\n",
      ">1, 400/468, d=0.000, g=7.239\n",
      ">1, 401/468, d=0.000, g=7.231\n",
      ">1, 402/468, d=0.000, g=7.240\n",
      ">1, 403/468, d=0.000, g=7.239\n",
      ">1, 404/468, d=0.000, g=7.251\n",
      ">1, 405/468, d=0.000, g=7.277\n",
      ">1, 406/468, d=0.000, g=7.274\n",
      ">1, 407/468, d=0.000, g=7.285\n",
      ">1, 408/468, d=0.000, g=7.263\n",
      ">1, 409/468, d=0.000, g=7.286\n",
      ">1, 410/468, d=0.000, g=7.300\n",
      ">1, 411/468, d=0.000, g=7.306\n",
      ">1, 412/468, d=0.000, g=7.302\n",
      ">1, 413/468, d=0.000, g=7.326\n",
      ">1, 414/468, d=0.000, g=7.320\n",
      ">1, 415/468, d=0.000, g=7.308\n",
      ">1, 416/468, d=0.000, g=7.302\n",
      ">1, 417/468, d=0.000, g=7.317\n",
      ">1, 418/468, d=0.000, g=7.312\n",
      ">1, 419/468, d=0.000, g=7.332\n",
      ">1, 420/468, d=0.000, g=7.356\n",
      ">1, 421/468, d=0.000, g=7.341\n",
      ">1, 422/468, d=0.000, g=7.379\n",
      ">1, 423/468, d=0.000, g=7.397\n",
      ">1, 424/468, d=0.000, g=7.344\n",
      ">1, 425/468, d=0.000, g=7.350\n",
      ">1, 426/468, d=0.000, g=7.374\n",
      ">1, 427/468, d=0.000, g=7.368\n",
      ">1, 428/468, d=0.000, g=7.394\n",
      ">1, 429/468, d=0.000, g=7.420\n",
      ">1, 430/468, d=0.000, g=7.377\n",
      ">1, 431/468, d=0.000, g=7.412\n",
      ">1, 432/468, d=0.000, g=7.369\n",
      ">1, 433/468, d=0.000, g=7.398\n",
      ">1, 434/468, d=0.000, g=7.406\n",
      ">1, 435/468, d=0.000, g=7.410\n",
      ">1, 436/468, d=0.000, g=7.432\n",
      ">1, 437/468, d=0.000, g=7.423\n",
      ">1, 438/468, d=0.000, g=7.454\n",
      ">1, 439/468, d=0.000, g=7.448\n",
      ">1, 440/468, d=0.000, g=7.463\n",
      ">1, 441/468, d=0.000, g=7.439\n",
      ">1, 442/468, d=0.000, g=7.428\n",
      ">1, 443/468, d=0.000, g=7.454\n",
      ">1, 444/468, d=0.000, g=7.472\n",
      ">1, 445/468, d=0.000, g=7.445\n",
      ">1, 446/468, d=0.000, g=7.467\n",
      ">1, 447/468, d=0.000, g=7.477\n",
      ">1, 448/468, d=0.000, g=7.482\n",
      ">1, 449/468, d=0.000, g=7.479\n",
      ">1, 450/468, d=0.000, g=7.500\n",
      ">1, 451/468, d=0.000, g=7.482\n",
      ">1, 452/468, d=0.000, g=7.526\n",
      ">1, 453/468, d=0.000, g=7.509\n",
      ">1, 454/468, d=0.000, g=7.499\n",
      ">1, 455/468, d=0.000, g=7.528\n",
      ">1, 456/468, d=0.000, g=7.539\n",
      ">1, 457/468, d=0.000, g=7.492\n",
      ">1, 458/468, d=0.000, g=7.558\n",
      ">1, 459/468, d=0.000, g=7.541\n",
      ">1, 460/468, d=0.000, g=7.552\n",
      ">1, 461/468, d=0.000, g=7.509\n",
      ">1, 462/468, d=0.000, g=7.546\n",
      ">1, 463/468, d=0.000, g=7.580\n",
      ">1, 464/468, d=0.000, g=7.553\n",
      ">1, 465/468, d=0.000, g=7.574\n",
      ">1, 466/468, d=0.000, g=7.579\n",
      ">1, 467/468, d=0.000, g=7.562\n",
      ">1, 468/468, d=0.000, g=7.600\n",
      ">2, 1/468, d=0.000, g=7.556\n",
      ">2, 2/468, d=0.000, g=7.577\n",
      ">2, 3/468, d=0.000, g=7.589\n",
      ">2, 4/468, d=0.000, g=7.606\n",
      ">2, 5/468, d=0.000, g=7.624\n",
      ">2, 6/468, d=0.000, g=7.643\n",
      ">2, 7/468, d=0.000, g=7.601\n",
      ">2, 8/468, d=0.000, g=7.618\n",
      ">2, 9/468, d=0.000, g=7.592\n",
      ">2, 10/468, d=0.000, g=7.637\n",
      ">2, 11/468, d=0.000, g=7.611\n",
      ">2, 12/468, d=0.000, g=7.647\n",
      ">2, 13/468, d=0.000, g=7.640\n",
      ">2, 14/468, d=0.000, g=7.640\n",
      ">2, 15/468, d=0.000, g=7.670\n",
      ">2, 16/468, d=0.000, g=7.639\n",
      ">2, 17/468, d=0.000, g=7.643\n",
      ">2, 18/468, d=0.000, g=7.641\n",
      ">2, 19/468, d=0.000, g=7.649\n",
      ">2, 20/468, d=0.000, g=7.690\n",
      ">2, 21/468, d=0.000, g=7.654\n",
      ">2, 22/468, d=0.000, g=7.682\n",
      ">2, 23/468, d=0.000, g=7.688\n",
      ">2, 24/468, d=0.000, g=7.688\n",
      ">2, 25/468, d=0.000, g=7.692\n",
      ">2, 26/468, d=0.000, g=7.684\n",
      ">2, 27/468, d=0.000, g=7.671\n",
      ">2, 28/468, d=0.000, g=7.678\n",
      ">2, 29/468, d=0.000, g=7.704\n",
      ">2, 30/468, d=0.000, g=7.740\n",
      ">2, 31/468, d=0.000, g=7.722\n",
      ">2, 32/468, d=0.000, g=7.741\n",
      ">2, 33/468, d=0.000, g=7.701\n",
      ">2, 34/468, d=0.000, g=7.715\n",
      ">2, 35/468, d=0.000, g=7.705\n",
      ">2, 36/468, d=0.000, g=7.751\n",
      ">2, 37/468, d=0.000, g=7.746\n",
      ">2, 38/468, d=0.000, g=7.753\n",
      ">2, 39/468, d=0.000, g=7.743\n",
      ">2, 40/468, d=0.000, g=7.717\n",
      ">2, 41/468, d=0.000, g=7.761\n",
      ">2, 42/468, d=0.000, g=7.756\n",
      ">2, 43/468, d=0.000, g=7.773\n",
      ">2, 44/468, d=0.000, g=7.777\n",
      ">2, 45/468, d=0.000, g=7.778\n",
      ">2, 46/468, d=0.000, g=7.787\n",
      ">2, 47/468, d=0.000, g=7.790\n",
      ">2, 48/468, d=0.000, g=7.782\n",
      ">2, 49/468, d=0.000, g=7.795\n",
      ">2, 50/468, d=0.000, g=7.795\n",
      ">2, 51/468, d=0.000, g=7.802\n",
      ">2, 52/468, d=0.000, g=7.776\n",
      ">2, 53/468, d=0.000, g=7.794\n",
      ">2, 54/468, d=0.000, g=7.810\n",
      ">2, 55/468, d=0.000, g=7.829\n",
      ">2, 56/468, d=0.000, g=7.842\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-5fd59a475f76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_real_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;31m# train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgan_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-5fd59a475f76>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(g_model, d_model, gan_model, dataset, latent_dim, n_epochs, n_batch)\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0my_gan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0;31m# train the gan model with latent inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m             \u001b[0mg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgan_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_gan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_gan\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m             \u001b[0;31m# summarize loss on this batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'>%d, %d/%d, d=%.3f, g=%.3f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbat_per_epo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, class_weight, sample_weight)\u001b[0m\n\u001b[1;32m   1046\u001b[0m         return self.model.train_on_batch(x, y,\n\u001b[1;32m   1047\u001b[0m                                          \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1048\u001b[0;31m                                          class_weight=class_weight)\n\u001b[0m\u001b[1;32m   1049\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m     def test_on_batch(self, x, y,\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1830\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1831\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1832\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1833\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1834\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2350\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2351\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2352\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2353\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from numpy import expand_dims\n",
    "from numpy import ones\n",
    "from numpy import zeros\n",
    "from numpy.random import rand\n",
    "from numpy.random import randint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Conv2D, Flatten, LeakyReLU, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import Conv2DTranspose\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from numpy import zeros\n",
    "from numpy.random import randn\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import Conv2DTranspose\n",
    "from keras.layers import LeakyReLU\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# load and prepare mnist training images\n",
    "def load_real_samples():\n",
    "    # load mnist dataset\n",
    "    (trainX, _), (_, _) = load_data()\n",
    "    # expand to 3d, e.g. add channels dimension\n",
    "    X = expand_dims(trainX, axis=3)\n",
    "    # convert from unsigned ints to floats\n",
    "    X = X.astype(float)\n",
    "    # scale from [0,255] to [0,1]\n",
    "    X = X / 255.0\n",
    "    return X\n",
    "\n",
    "def real_sampler(data, num_samples):\n",
    "    #random samples\n",
    "    ran = randint(num_samples, data.shape[0])\n",
    "    X = data[ran-num_samples:ran]\n",
    "    y = zeros((num_samples, 1))\n",
    "    return(X,y)\n",
    "\n",
    "def fake_sampler(num_samples):\n",
    "    # generate uniform random numbers in [0,1]\n",
    "    X = rand(28 * 28 * num_samples)\n",
    "    # reshape into a batch of grayscale images\n",
    "    X = X.reshape((num_samples, 28, 28, 1))\n",
    "    # generate 'fake' class labels (0)\n",
    "    y = zeros((num_samples, 1))\n",
    "    return X, y\n",
    "\n",
    "#create the descriminator model\n",
    "\n",
    "img_shape = (28,28,1)\n",
    "def descriminator():\n",
    "    model= Sequential()\n",
    "    model.add(Conv2D(64, (3,3), strides=(2,2), padding= 'same', input_shape=img_shape))\n",
    "    model.add(LeakyReLU(alpha=.2))\n",
    "    model.add(Dropout(.2))\n",
    "    model.add(Conv2D(32, (3,3), strides=(2,2), padding= 'same'))\n",
    "    model.add(LeakyReLU(alpha=.2))\n",
    "    model.add(Dropout(.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer=Adam(lr=.0002), metrics=['accuracy'])\n",
    "    return model \n",
    "\n",
    "#train the descrinator with fake samples from a random distribution not a generator\n",
    "def train_discriminator(model, data, num_iter, num_batch):\n",
    "    half_batch = int(num_batch/2)\n",
    "    for i in range(num_iter):\n",
    "        # get real samples\n",
    "        X_real, y_real = real_sampler(data, half_batch)\n",
    "        #train discriminator on real samples\n",
    "        _, real_acc = model.train_on_batch(X_real, y_real)\n",
    "        # get fake\n",
    "        X_fake, y_fake = fake_sampler(half_batch)\n",
    "        # train discrimanor on fake\n",
    "        _, fake_acc = model.train_on_batch(X_fake, y_fake)\n",
    "        #perfomrance\n",
    "        print('>%d real=%.0f%% fake=%.0f%%' % (i+1, real_acc*100, fake_acc*100))\n",
    "        \n",
    "        \n",
    "# define the standalone generator model\n",
    "def generator(latent_dim):\n",
    "    model = Sequential()\n",
    "    # foundation for 7x7 image\n",
    "    n_nodes = 256 * 7 * 7\n",
    "    model.add(Dense(n_nodes, input_dim=latent_dim))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Reshape((7, 7, 256)))\n",
    "    # upsample to 14x14\n",
    "    model.add(Conv2DTranspose(256, (4,4), strides=(2,2), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    # upsample to 28x28\n",
    "    model.add(Conv2DTranspose(256, (4,4), strides=(2,2), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Conv2D(1, (7,7), activation='sigmoid', padding='same'))\n",
    "    return model\n",
    "\n",
    "# Generatr some points in the latent space to seed the fake samples\n",
    "\n",
    "def gen_latent(latent_dim, num_samples):\n",
    "    # generate points in the latent space\n",
    "    x_input = randn(latent_dim * num_samples)\n",
    "    # reshape into a batch of inputs for the network\n",
    "    x_input = x_input.reshape(num_samples, latent_dim)\n",
    "    return x_input\n",
    "\n",
    "\n",
    "## generate fake samples using the generator model\n",
    "def generate_fake_samples(gen_model, latent_dim, num_samples):\n",
    "    # generate points in latent space\n",
    "    x_input = gen_latent(latent_dim, num_samples)\n",
    "    # predict outputs\n",
    "    X = gen_model.predict(x_input)\n",
    "    # create 'fake' class labels (0)\n",
    "    y = zeros((num_samples, 1))\n",
    "    return X, y\n",
    "\n",
    "\n",
    "# define the combined generator and discriminator model, for updating the generator\n",
    "def gan(g_model, d_model):\n",
    "    # make weights in the discriminator not trainable\n",
    "    d_model.trainable = False\n",
    "    # connect them\n",
    "    model = Sequential()\n",
    "    # add generator\n",
    "    model.add(g_model)\n",
    "    # add the discriminator\n",
    "    model.add(d_model)\n",
    "    # compile model\n",
    "    opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt)\n",
    "    return model\n",
    "\n",
    "        \n",
    "# train the GAN\n",
    "def train(g_model, d_model, gan_model, dataset, latent_dim, n_epochs=100, n_batch=128):\n",
    "    bat_per_epo = int(dataset.shape[0] / n_batch)\n",
    "    half_batch = int(n_batch / 2)\n",
    "    #training loop of epochs\n",
    "    for i in range(n_epochs):\n",
    "        # enumerate batches over the training set\n",
    "        for j in range(bat_per_epo):\n",
    "            # get randomly selected 'real' samples\n",
    "            X_real, y_real = real_sampler(dataset, half_batch)\n",
    "            # fake samples using the generator model\n",
    "            X_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n",
    "            # merge samples to create training set for the discriminator\n",
    "            X, y = np.vstack((X_real, X_fake)), np.vstack((y_real, y_fake))\n",
    "            # train the  discriminator model weights on batch\n",
    "            d_loss, _ = d_model.train_on_batch(X, y)\n",
    "            # randomnly choose points in latent space as input for the generator\n",
    "            X_gan = gen_latent(latent_dim, n_batch)\n",
    "            # create false positives for the fake samples to fool the generator\n",
    "            y_gan = ones((n_batch, 1))\n",
    "            # train the gan model with latent inputs\n",
    "            g_loss = gan_model.train_on_batch(X_gan, y_gan)\n",
    "            # summarize loss on this batch\n",
    "            print('>%d, %d/%d, dis=%.3f, gen=%.3f' % (i+1, j+1, bat_per_epo, d_loss, g_loss))\n",
    "            # evaluate the model performance, sometimes\n",
    "            if (i+1) % 10 == 0:\n",
    "                model_summary(i, g_model, d_model, dataset, latent_dim)\n",
    "                \n",
    "def model_summary(epoc, g_model, d_model, dataset, latent_dim, n_samples=100):\n",
    "    #prepare real\n",
    "    X_real, y_real = real_sampler(dataset, n_samples)\n",
    "    #evaluate discriminator on real samples\n",
    "    _, acc_real = d_model.evaluate(X_real, y_real) \n",
    "    #prepare fake examples\n",
    "    X_fake, y_fake = fake_sampler(g_model, latent_dim, num_samples)\n",
    "    #evaluate discriminator on fake samples\n",
    "    _, acc_fake = d_model.evaluate(X_fake, y_fake, verbose=0)\n",
    "    # summarize discriminator performance\n",
    "    print('>Accuracy real: %.0f%%, fake: %.0f%%' % (acc_real*100, acc_fake*100))\n",
    "    # save plot\n",
    "    save_plot(x_fake, epoch)\n",
    "    # save the generator model tile file\n",
    "    cwd = os.getcwd()\n",
    "    file = 'saved/generator_model_%03d.h5' % (epoch + 1)\n",
    "    filename = os.path.join(cwd,filename)\n",
    "    g_model.save(filename)\n",
    "\n",
    "def save_plot(examples, epoch, n=10):\n",
    "    # plot images\n",
    "    for i in range(n * n):\n",
    "        # define subplot\n",
    "        pyplot.subplot(n, n, 1 + i)\n",
    "        # turn off axis\n",
    "        pyplot.axis('off')\n",
    "        # plot raw pixel data\n",
    "        pyplot.imshow(examples[i, :, :, 0], cmap='gray_r')\n",
    "        # save plot to file\n",
    "        cwd = os.getcwd()\n",
    "        filename = 'saved/generated_plot_e%03d.png' % (epoch+1)\n",
    "        file = os.path.join(cwd,filename)\n",
    "        pyplot.savefig(file)\n",
    "        pyplot.close()\n",
    "\n",
    "# size of the latent space\n",
    "latent_dim = 100\n",
    "# create the discriminator\n",
    "d_model = descriminator()\n",
    "# create the generator\n",
    "g_model = generator(latent_dim)\n",
    "# create the gan\n",
    "gan_model = gan(g_model, d_model)\n",
    "# load image data\n",
    "dataset = load_real_samples()\n",
    "# train model\n",
    "train(g_model, d_model, gan_model, dataset, latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
